<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Value Flows">
  <meta name="keywords" content="Batch Online RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script> -->
  <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>

  <title>Value Flows</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <style>
    /* Tighter vertical rhythm */
    .section.compact { padding-top: 1.25rem; padding-bottom: 1.25rem; }
    /* .hero.is-small .hero-body { padding-top: 1.25rem; padding-bottom: 1.25rem; } */

    /* Reduce heading spacing inside sections */
    /* .section.compact .title { margin-bottom: 1rem; } */
    .section.compact .title.is-3 { margin-top: 0.1rem; }

    /* Kill extra trailing space from last elements */
    .section.compact .content > :last-child,
    .section.compact .publication-image:last-child,
    .section.compact .video-container:last-child { margin-bottom: 0; }
    .publication-title { font-size: 3rem; font-weight: bold; }
    .publication-authors { margin-top: 1rem; }
    .author-block { display: inline-block; margin: 0 0.5rem; }
    .publication-links { margin-top: 1.5rem; }
    .link-block { margin: 0.25rem; }
    .content.has-text-justified { text-align: justify; }
    .publication-image { margin: 2rem 0; }
    hr.rounded { border: 2px solid #e5e5e5; border-radius: 5px; margin: 3rem 0; }
    .hidden { display: none; }
    .toggle-link { color: #1f77b4; cursor: pointer; display: inline-block; margin-bottom: 1rem; }
    .toggle-link:hover { color: #1f77b4; }
    .table-image-container { margin: 1.5rem 0; }
    .table-image-container img { width: 100%; height: auto; }
    .charts-container { margin: 1.5rem 0; }
    .charts-container img { width: 100%; height: auto; }
    .caption-text { margin: 1rem 0; font-style: italic; }
    .benefits-list { margin: 1rem 0 1rem 2rem; }
    .benefits-list li { margin: 0.5rem 0; }
    .task-selector { margin: 2rem 0; }
    .task-selector .button { margin: 0.25rem; }
    .video-container { margin: 1rem 0; }
    .video-title { font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem; }
    .video-wrapper { display: flex; justify-content: center; gap: 1rem; flex-wrap: wrap; }
    .video-item { flex: 1; min-width: 250px; max-width: 2048px; }
    .video-item video { width: 100%; height: auto; border-radius: 8px; }
    .video-label { text-align: center; margin-top: 0.5rem; font-weight: bold; }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Value Flows</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dblp.org/pid/323/8972.html">Perry Dong*</a><sup>1</sup>&emsp;
            </span>
            <span class="author-block">
              <a href="https://chongyi-zheng.github.io/">Chongyi Zheng*</a><sup>2</sup>&emsp;
            </span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><sup>1</sup>&emsp;
            </span>
            <span class="author-block">
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a><sup>1</sup>&emsp;
            </span>
            <span class="author-block">
              <a href="https://ben-eysenbach.github.io/">Benjamin Eysenbach</a><sup>2</sup>&emsp;</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Stanford University&emsp;
              <sup>2</sup>PrincetonÂ University
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              (*Equal contribution)
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2510.07650" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
                <a href="https://github.com/chongyi-zheng/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-code"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
While most reinforcement learning methods today flatten the distribution of future returns to a single scalar value, distributional RL methods exploit the return distribution to provide stronger learning signals and to enable applications in exploration and safe RL. While the predominant method for estimating the return distribution is by modeling it as a categorical distribution over discrete bins or estimating a finite number of quantiles, such approaches leave unanswered questions about the fine-grained structure of the return distribution and about how to distinguish states with high return uncertainty for decision-making. The key idea in this paper is to use modern, flexible flow-based models to estimate the full future return distributions and identify those states with high return variance. We do so by formulating a new flow-matching objective that generates probability density paths satisfying the distributional Bellman equation. Building upon the learned flow models, we estimate the return uncertainty of distinct states using a new flow derivative ODE. We additionally use this uncertainty information to prioritize learning a more accurate return estimation on certain transitions. We compare our method (Value Flows) with prior methods in the offline and online-to-online settings. Experiments on 37 state-based and 25 image-based benchmark tasks demonstrate that Value Flows achieves a 1.3x improvement on average in success rates.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="publication-image" align="center">
          <img src="static/images/value_flows.png" alt="Teaser image" width="60%">
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section compact">
  <div class="publication-image" align="center">
    <img src="static/images/value_flows.png" alt="Teaser image" width="50%">
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-3">Overview</h3>

        <!-- <div class="publication-image" align="center">
          <img src="static/images/value_flows.png" alt="Teaser image" width="60%">
        </div> -->
        
        <div class="content has-text-justified">
          <p>
          <!-- <b>Value Flows</b> is an RL algorithm that models the full return distribution via flow-matching and enforces Bellman consistency at every transition. -->
          While many of the recent successes in reinforcement learning (RL) have focused on estimating future returns as a single scalar, distributional RL methods provide stronger learning signals through exploit the full return distributions. <b>Value Flows</b> is a distributional RL algorithm that models the full return distribution via flow-matching and enforces Bellman consistency at every transition.
          <!-- Prior methods for estimating the return distribution model it as a categorical distribution over discrete bins or estimating a finite number of quantiles.</p> -->
          <!-- <p>We propose<b> Value Flows:</b> </p> -->
          <!-- <ul class="benefits-list">
            <li>Value Flows is a framework for modeling the return distribution using modern, flexible flow-based models.</li>
            <li>Value Flows optimizes a distributional flow-matching objective that generates probability density paths satisfying the distributional Bellman equation automatically, which can be used to estimate the full future return distributions.</li>
          </ul> -->
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section compact">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-3">Key Ideas</h3>   
        <div class="content has-text-justified">
        <!-- <a href="#" class="toggle-link" id="toggleTable3">
          Click to see the full algorithm
        </a>

        <div id="tableContainer3" class="hidden">
          <div class="table-image-container">
            <img src="static/images/algorith.png" alt="Alg">
          </div>
        </div>
        
        <br><br> -->
        <p>Value Flows consists of three main components: <em>(1)</em> the vector field estimating the return distribution, <em>(2)</em> the confidence weight prioritizing learning the return distribution at uncertain transitions, and <em>(3)</em> the flow policy for selecting actions.</p>
        <p>
          <b>Return vector fields: </b>
          We formulate a distributional flow-matching objective to learn return vector fields satisfying the distributional Bellman equation automatically. Additionally, we including a regularization term for stability in practice. 
        </p>
        <p>
          <b>Confidence weights: </b> Our confidence weights prioritize learning a more accurate return distribution at transitions with higher return variance, as estimated by the return vector field.
        </p>
        <p>
          <b>Policy extractions: </b>
          <ul class="benefits-list">
            <li>For offline RL, we use rejection sampling to maximize Q estimates while implicitly imposing a KL constraint toward a fixed flow behavioral cloning (BC) policy.</li>
            <li>For online fine-tuning in offline-to-online RL, we learn a stochastic one-step policy to maximize the Q estimates while distilling it toward the fixed BC flow policy. </li>
          </ul>
        </p>
        </div>
    </div>
  </div>
</section>

<section class="section compact">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="video-container">
          <h3 class="title is-3">Visualizing Return Distributions</h3>
          <!-- <h3 class="video-title">Visualizing Return Distributions</h3> -->
          <div class="video-wrapper">
            <div class="video-item">
              <video id="vfHistograms" playsinline muted autoplay loop controls poster="static/images/value_flows.png">
                <source src="static/videos/value_flows_histograms.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <!-- <div class="video-label">Return Histograms</div> -->
            </div>
          </div>
          <div class="content has-text-justified">
            <p>
              <em>Column 1:</em> The policy completes the task of closing the window and closing the drawer using the buttons to lock and unlock them. 
            </p>
            <p>
              <em>Column 2:</em> C51 discretizes the return distribution and predicts a noisy multi-modal distribution.
            </p>
            <p>
              <em>Column 3:</em> CODAC uses a finite number of quantiles to represent the return distribution and collapses to a single return mode.
            </p>
            <p>
              <em>Column 4 and 5:</em> Value Flows infers a smooth return histogram resembling the ground-truth return distribution, achieving \(3\times\) lower 1-Wasserstein distance averaged across time than alternative methods.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section compact">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-3">Offline and offline-to-online evaluations</h3>  
        <h4 class="title is-4">Offline RL</h4>
        <div class="content has-text-justified">
        <a href="#" class="toggle-link" id="toggleTable">
          Click to see the full table (<span id="taskCount">62</span> tasks)
        </a>
        <div id="tableContainer" class="hidden">
          <figure class="table-image-container">
            <img src="static/images/results_table.png" alt="Offline results table">
          </figure>
        </div>
        <figure class="table-image-container">
          <img src="static/images/results_table_avg.png" alt="Offline RL results table aggregated across domains">
          <figcaption class="title is-6" style="font-style: normal;">
            Offline RL results averaged over 8 seeds on 62 continous control tasks from OGBench and D4RL.
          </figcaption>
        </figure>
        <p>
          <ul class="benefits-list">
            <li>Value flows matches or outperforms all baselines on \(9\) out of \(11\) domains. </li>
            <li>On those more challenging state-based tasks, Value Flows achieves \( 1.6 \times\) higher success rates than the best performing baseline.</li>
            <li>Value Flows is able to outperform the best baseline by \(1.24\times\) using RGB images directly.</li>
          </ul>
        </p>

        <h4 class="title is-4">Offline-to-online RL</h4>
        <!-- <div class="table-image-container" width="10%">
            <img src="static/images/offline_to_online_together.jpg" alt="Online RL results table">
          </div> -->
        <a href="#" class="toggle-link" id="toggleTable2">
          Click to see the full table (<span id="taskCount2">6</span> tasks)
        </a>
        <div id="tableContainer2" class="hidden">
          <div class="table-image-container">
            <img src="static/images/online_results_table.png" alt="Online RL results table">
          </div>
          <div class="charts-container">
            <img src="static/images/offline_to_online.jpg" alt="Online training curves">
          </div>
          <figcaption class="title is-6" style="font-style: normal;">
            Offline-to-online RL results averaged over 8 seeds on 6 continous control tasks from OGBench.
          </figcaption>
        </div>
        <div class="publication-image" align="center">
          <img src="static/images/offline_to_online_together.jpg" alt="Online RL results table" width="60%">
        </div>
        <p>
          <ul class="benefits-list">
            <li>Value Flows continues outperforming prior state-of-the-art RL and distributional RL methods using online interactions.</li>
            <li>Value Flows can be used without any modifications to the vector field objective.</li>
          </ul>
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section compact">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h4 class="title is-4">The Key Components of Value Flows</h4>
        <div class="container">
          <div class="content has-text-justified">
            <div class="image-wrapper" style="display: flex; gap: 12px; align-items: flex-start;">
              <div class="image-container" style="flex: 1; text-align: center;">
                  <img src="static/images/cube-double_puzzle-3x3_lambda.png" style="width: 99%;">
                  <figcaption class="title is-6" style="font-style: normal; text-align: center;">
                    Regularizing the flow-matching loss is important.
                  </figcaption>
              </div>
              <div class="image-container" style="flex: 1; text-align: center;">
                  <img src="static/images/cube-double_puzzle-3x3_tau.png" style="width: 100%;">
                  <figcaption class="title is-6" style="font-style: normal; text-align: center;">
                    Reweighing the flow-matching objective boosts success rates.
                  </figcaption>
              </div>
            </div>
            <!-- <p>
              <ul class="benefits-list">
                <li>Value Flows is robust to common hyperparameters such as number of flow steps and number of candidates in rejection sampling.</li>
                <li>One only needs to tune the regularization loss coefficient and confidence weight. </li>
              </ul>
            </p> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section compact" id="BibTeX">
  <div class="container is-max-desktop content">
    <h3 class="title">BibTeX</h3>
    <pre><code>@misc{dong2025value,
  title={Value Flows}, 
  author={Perry Dong and Chongyi Zheng and Chelsea Finn and Dorsa Sadigh and Benjamin Eysenbach},
  year={2025},
  eprint={2510.07650},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2510.07650},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>,
            courtesy of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://peract.github.io/">PerAct</a>, and <a href="https://emprise.cs.cornell.edu/flair/">FLAIR</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  function changeTask(taskName, startVideo, midVideo, lastVideo) {
    const title = document.getElementById('video-title');
    const player1 = document.getElementById('task-video-player-1');
    const player2 = document.getElementById('task-video-player-2');
    const player3 = document.getElementById('task-video-player-3');
    const item2 = document.getElementById('task-video-item-2');
    
    title.textContent = taskName.charAt(0).toUpperCase() + taskName.slice(1);
    
    player1.src = startVideo;
    player1.load();
    
    if (midVideo) {
      player2.src = midVideo;
      player2.load();
      item2.style.display = 'block';
    } else {
      item2.style.display = 'none';
    }
    
    player3.src = lastVideo;
    player3.load();
  }

  document.addEventListener('DOMContentLoaded', function() {
    const toggleLink = document.getElementById('toggleTable');
    const tableContainer = document.getElementById('tableContainer');
    
    if (toggleLink && tableContainer) {
      let isExpanded = false;

      toggleLink.addEventListener('click', function(e) {
        e.preventDefault();
        isExpanded = !isExpanded;
        
        if (isExpanded) {
          tableContainer.classList.remove('hidden');
          toggleLink.innerHTML = 'Click to hide the full table (<span id="taskCount">62</span> tasks)';
        } else {
          tableContainer.classList.add('hidden');
          toggleLink.innerHTML = 'Click to see the full table (<span id="taskCount">62</span> tasks)';
        }
      });
    }

    const toggleLink2 = document.getElementById('toggleTable2');
    const tableContainer2 = document.getElementById('tableContainer2');
    
    if (toggleLink2 && tableContainer2) {
      let isExpanded2 = false;

      toggleLink2.addEventListener('click', function(e) {
        e.preventDefault();
        isExpanded2 = !isExpanded2;
        
        if (isExpanded2) {
          tableContainer2.classList.remove('hidden');
          toggleLink2.innerHTML = 'Click to hide the full table (<span id="taskCount2">6</span> tasks)';
        } else {
          tableContainer2.classList.add('hidden');
          toggleLink2.innerHTML = 'Click to see the full table (<span id="taskCount2">6</span> tasks)';
        }
      });
    }


    const toggleLink3 = document.getElementById('toggleTable3');
    const tableContainer3 = document.getElementById('tableContainer3');
    
    if (toggleLink3 && tableContainer3) {
      let isExpanded3 = false;

      toggleLink3.addEventListener('click', function(e) {
        e.preventDefault();
        isExpanded3 = !isExpanded3;
        
        if (isExpanded3) {
          tableContainer3.classList.remove('hidden');
          toggleLink3.innerHTML = 'Click to hide the full table (<span id="taskCount3">6</span> tasks)';
        } else {
          tableContainer3.classList.add('hidden');
          toggleLink3.innerHTML = 'Click to see the full table (<span id="taskCount3">6</span> tasks)';
        }
      });
    }


  });
</script>

</body>
</html>
